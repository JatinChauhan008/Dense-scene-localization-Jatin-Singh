{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 1: Install Dependencies\n"
      ],
      "metadata": {
        "id": "1GC4ohtZ_Uey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('[1/6] Installing required packages...')\n",
        "!pip install -q torch torchvision transformers datasets accelerate ftfy wandb\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q pillow tqdm scikit-learn\n",
        "!pip install -q pycocotools supervision\n",
        "!pip install -q timm\n",
        "print('Packages installed.')"
      ],
      "metadata": {
        "id": "9ltgBHcz9FYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 2: Imports and Setup\n"
      ],
      "metadata": {
        "id": "MGXpdyVx_f9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n [2/6] Setting up environment...')\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import os, glob, json, random, math, re\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from functools import partial\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    AutoProcessor, AutoModelForZeroShotObjectDetection,\n",
        "    AutoTokenizer, get_linear_schedule_with_warmup\n",
        ")\n",
        "from torchvision.ops import nms, box_iou\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {DEVICE}')\n",
        "\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/Model_Training\"\n",
        "IMAGES_DIR = os.path.join(DRIVE_ROOT, \"images\")\n",
        "ANNOTATIONS_DIR = os.path.join(DRIVE_ROOT, \"annotations\")\n",
        "SENTENCES_DIR = os.path.join(DRIVE_ROOT, \"sentences\")\n",
        "OUT_DIR = os.path.join(DRIVE_ROOT, \"training_output\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "print('Attempting to unmount drive to ensure a clean connection...')\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "    print('Drive unmounted successfully.')\n",
        "except Exception as e:\n",
        "    print(f\"No drive to unmount or error during unmount: {e}\")\n",
        "\n",
        "print('Mounting Google Drive...')\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print('Drive mounted.')\n",
        "\n",
        "print(f\"Ensuring output directory exists: {OUT_DIR}\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "GSwK7d8C89Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CELL 3: Annotation Parsing Functions\n"
      ],
      "metadata": {
        "id": "ympAY5YY_rhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n [3/6] Setting up annotation parsers...')\n",
        "\n",
        "def parse_pascal_voc_xml(xml_folder, images_dir, class_map=None, keep_scene_without_box=False):\n",
        "    \"\"\"Parse Pascal-VOC xmls. Handles <nobndbox> tags and returns list of items.\"\"\"\n",
        "    xml_files = glob.glob(os.path.join(xml_folder, '*.xml'))\n",
        "    items = []\n",
        "    for xf in tqdm(xml_files, desc=\"Parsing XMLs\"):\n",
        "        try:\n",
        "            tree = ET.parse(xf)\n",
        "            root = tree.getroot()\n",
        "        except Exception as e:\n",
        "            print(f'Failed parse {xf}: {e}')\n",
        "            continue\n",
        "\n",
        "        fname_tag = root.find('filename')\n",
        "        filename = fname_tag.text.strip() if fname_tag is not None else os.path.splitext(os.path.basename(xf))[0] + '.jpg'\n",
        "        image_path = os.path.join(images_dir, filename)\n",
        "\n",
        "        for obj in root.findall('object'):\n",
        "            name_tag = obj.find('name')\n",
        "            name = name_tag.text.strip() if name_tag is not None else 'unknown'\n",
        "            nobnd = obj.find('nobndbox')\n",
        "            bnd = obj.find('bndbox')\n",
        "\n",
        "            if nobnd is not None and nobnd.text.strip() in ('1','true','True'):\n",
        "                if keep_scene_without_box:\n",
        "                    phrase = class_map.get(name, f'class_{name}') if class_map else (name if not name.isdigit() else f'class_{name}')\n",
        "                    items.append({'image_path': image_path, 'phrase': phrase, 'bbox': None})\n",
        "                continue\n",
        "\n",
        "            if bnd is None:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                xmin = float(bnd.find('xmin').text)\n",
        "                ymin = float(bnd.find('ymin').text)\n",
        "                xmax = float(bnd.find('xmax').text)\n",
        "                ymax = float(bnd.find('ymax').text)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            x = int(round(xmin)); y = int(round(ymin))\n",
        "            w = int(round(xmax - xmin)); h = int(round(ymax - ymin))\n",
        "            if w <= 0 or h <= 0: continue\n",
        "\n",
        "            phrase = class_map.get(name, f'class_{name}') if class_map else (name if not name.isdigit() else f'class_{name}')\n",
        "            items.append({'image_path': image_path, 'phrase': phrase, 'bbox': [x,y,w,h]})\n",
        "\n",
        "    print(f'Parsed {len(items)} objects from {len(xml_files)} xml files')\n",
        "    return items\n",
        "\n",
        "def extract_sentence_phrases_from_text(text):\n",
        "    \"\"\"Extract phrases from sentence annotation files.\"\"\"\n",
        "    pattern = re.compile(r'\\[/EN#(\\d+)\\/[^\\s\\]]+\\s([^\\]]+)\\]')\n",
        "    matches = pattern.findall(text)\n",
        "    out = {}\n",
        "    for idx, phrase in matches:\n",
        "        out.setdefault(idx, []).append(phrase.strip())\n",
        "    return out\n",
        "\n",
        "def parse_and_load_annotations():\n",
        "    \"\"\"Parse all annotations and create cleaned dataset.\"\"\"\n",
        "    all_items = []\n",
        "\n",
        "\n",
        "    if os.path.exists(ANNOTATIONS_DIR):\n",
        "        xmls = glob.glob(os.path.join(ANNOTATIONS_DIR,'*.xml'))\n",
        "        if xmls:\n",
        "            parsed = parse_pascal_voc_xml(ANNOTATIONS_DIR, IMAGES_DIR)\n",
        "            all_items.extend(parsed)\n",
        "\n",
        "\n",
        "    sentence_map = {}\n",
        "    if os.path.exists(SENTENCES_DIR):\n",
        "        text_files = glob.glob(os.path.join(SENTENCES_DIR, '*'))\n",
        "        for tf in tqdm(text_files, desc=\"Parsing Sentences\"):\n",
        "            try:\n",
        "                with open(tf, 'r', encoding='utf-8') as f:\n",
        "                    txt = f.read()\n",
        "                sm = extract_sentence_phrases_from_text(txt)\n",
        "                for k,v in sm.items():\n",
        "                    sentence_map.setdefault(k,[]).extend(v)\n",
        "            except Exception as e:\n",
        "                print(f'Could not read {tf}: {e}')\n",
        "\n",
        "\n",
        "    if sentence_map:\n",
        "        id_to_phrase = {}\n",
        "        for k,v in sentence_map.items():\n",
        "            counts = {}\n",
        "            for p in v:\n",
        "                counts[p] = counts.get(p,0)+1\n",
        "            best = sorted(counts.items(), key=lambda x:-x[1])[0][0]\n",
        "            id_to_phrase[k] = best\n",
        "\n",
        "        for it in all_items:\n",
        "            ph = it['phrase']\n",
        "            m = re.match(r'class_(\\d+)', str(ph))\n",
        "            if m:\n",
        "                iid = m.group(1)\n",
        "                if iid in id_to_phrase:\n",
        "                    it['phrase'] = id_to_phrase[iid]\n",
        "\n",
        "\n",
        "    cleaned = []\n",
        "    image_paths_checked = set()\n",
        "    for it in all_items:\n",
        "        if it['image_path'] not in image_paths_checked:\n",
        "            if not os.path.exists(it['image_path']):\n",
        "                print(f\"Warning: Image not found, skipping item: {it['image_path']}\")\n",
        "                continue\n",
        "            image_paths_checked.add(it['image_path'])\n",
        "\n",
        "        if it['bbox'] is None: continue\n",
        "        x,y,w,h = it['bbox']\n",
        "        it['bbox'] = [int(round(x)), int(round(y)), int(round(w)), int(round(h))]\n",
        "        cleaned.append(it)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "print(' Annotation parsers ready.')"
      ],
      "metadata": {
        "id": "G9iSCggQ9MGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4: Dataset Classes"
      ],
      "metadata": {
        "id": "iOmfw5nw_zjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n [4/6] Setting up dataset classes...')\n",
        "\n",
        "class CLIPReRankDataset(Dataset):\n",
        "    \"\"\"Dataset for training CLIP re-ranker.\"\"\"\n",
        "    def __init__(self, items, processor, num_negatives=3, crop_size=224):\n",
        "        self.items = items\n",
        "        self.processor = processor\n",
        "        self.num_neg = num_negatives\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def _crop_from_bbox(self, image, bbox, pad=0.15):\n",
        "        x,y,w,h = bbox\n",
        "        W,H = image.size\n",
        "        pad_w = int(w*pad); pad_h = int(h*pad)\n",
        "        x1 = max(0, x-pad_w); y1 = max(0, y-pad_h)\n",
        "        x2 = min(W, x+w+pad_w); y2 = min(H, y+h+pad_h)\n",
        "        return image.crop((x1,y1,x2,y2)).resize((self.crop_size,self.crop_size))\n",
        "\n",
        "    def _random_neg_crop(self, image):\n",
        "        W,H = image.size\n",
        "        for _ in range(10):\n",
        "            w = random.randint(40, min(W//2,200))\n",
        "            h = random.randint(40, min(H//2,200))\n",
        "            x = random.randint(0, max(0, W-w))\n",
        "            y = random.randint(0, max(0, H-h))\n",
        "            return image.crop((x,y,x+w,y+h)).resize((self.crop_size,self.crop_size))\n",
        "        return image.resize((self.crop_size,self.crop_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]\n",
        "        try:\n",
        "            img = Image.open(item['image_path']).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open {item['image_path']}, returning None. Error: {e}\")\n",
        "            return None\n",
        "\n",
        "        pos = self._crop_from_bbox(img, item['bbox'])\n",
        "        negs = []\n",
        "        for _ in range(self.num_neg):\n",
        "            if random.random() < 0.5:\n",
        "                negs.append(self._random_neg_crop(img))\n",
        "            else:\n",
        "                other = random.choice(self.items)\n",
        "                other_img = Image.open(other['image_path']).convert('RGB')\n",
        "                negs.append(self._random_neg_crop(other_img))\n",
        "\n",
        "        proc = self.processor(\n",
        "            text=item['phrase'],\n",
        "            images=[pos]+negs,\n",
        "            return_tensors='pt',\n",
        "            padding=\"max_length\",\n",
        "            max_length=77,\n",
        "            truncation=True\n",
        "        )\n",
        "        return {k: v.squeeze(0) if v.dim()>0 else v for k,v in proc.items()}\n",
        "\n",
        "class GroundingDINODataset(Dataset):\n",
        "    \"\"\"Dataset for training Grounding DINO.\"\"\"\n",
        "    def __init__(self, items):\n",
        "        self.items = items\n",
        "        self.image_groups = {}\n",
        "        for item in items:\n",
        "            img_path = item['image_path']\n",
        "            self.image_groups.setdefault(img_path, []).append(item)\n",
        "        self.image_list = list(self.image_groups.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_list[idx]\n",
        "        items_for_image = self.image_groups[img_path]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Could not open {img_path}, returning None. Error: {e}\")\n",
        "            return None\n",
        "\n",
        "        W, H = image.size\n",
        "\n",
        "        boxes = []\n",
        "        phrases_for_valid_boxes = []\n",
        "        for item in items_for_image:\n",
        "            x, y, w, h = item['bbox']\n",
        "            phrase = item['phrase']\n",
        "\n",
        "            if w <= 0 or h <= 0:\n",
        "                continue\n",
        "\n",
        "\n",
        "            norm_w, norm_h = w / W, h / H\n",
        "            center_x = (x / W) + (norm_w / 2)\n",
        "            center_y = (y / H) + (norm_h / 2)\n",
        "\n",
        "\n",
        "            if norm_w < 1e-4 or norm_h < 1e-4:\n",
        "                continue\n",
        "\n",
        "\n",
        "            center_x = max(0.0, min(center_x, 1.0))\n",
        "            center_y = max(0.0, min(center_y, 1.0))\n",
        "            norm_w = max(0.0, min(norm_w, 1.0))\n",
        "            norm_h = max(0.0, min(norm_h, 1.0))\n",
        "\n",
        "\n",
        "            boxes.append([center_x, center_y, norm_w, norm_h])\n",
        "            phrases_for_valid_boxes.append(phrase)\n",
        "\n",
        "\n",
        "        if not boxes:\n",
        "            return None\n",
        "\n",
        "\n",
        "        unique_phrases = sorted(list(set(phrases_for_valid_boxes)))\n",
        "        text_prompt = \". \".join(unique_phrases) + \".\"\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"text\": text_prompt,\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float),\n",
        "            \"phrases\": phrases_for_valid_boxes,\n",
        "            \"unique_phrases\": unique_phrases\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"General collate function that handles None items.\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch: return None\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "def grounding_dino_collate_fn(batch, processor):\n",
        "    \"\"\"Custom collate function to handle batch processing for Grounding DINO.\"\"\"\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch: return None\n",
        "\n",
        "    images = [item['image'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "    target_boxes = [item['boxes'] for item in batch]\n",
        "    target_phrases = [item['phrases'] for item in batch]\n",
        "    unique_phrases_list = [item['unique_phrases'] for item in batch] # âœ… FIX: Get the unique phrases\n",
        "\n",
        "    inputs = processor(\n",
        "        images=images,\n",
        "        text=texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    inputs['target_boxes'] = target_boxes\n",
        "    inputs['target_phrases'] = target_phrases\n",
        "    inputs['text'] = texts\n",
        "    inputs['unique_phrases'] = unique_phrases_list\n",
        "\n",
        "    return inputs\n",
        "\n",
        "print(' Dataset classes ready.')"
      ],
      "metadata": {
        "id": "xSwxo1Qe9ajE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 5: Model Classes and Training Functions"
      ],
      "metadata": {
        "id": "OPWMJWty_-0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n [5/6] Setting up models and training functions...')\n",
        "\n",
        "class ReRanker(nn.Module):\n",
        "    \"\"\"CLIP-based re-ranker model.\"\"\"\n",
        "    def __init__(self, clip_model, proj_dim=256, train_image_encoder=False):\n",
        "        super().__init__()\n",
        "        self.clip = clip_model\n",
        "        self.train_image_encoder = train_image_encoder\n",
        "        self.img_proj = nn.Sequential(\n",
        "            nn.Linear(self.clip.visual_projection.out_features, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(proj_dim, proj_dim)\n",
        "        )\n",
        "        self.txt_proj = nn.Sequential(\n",
        "            nn.Linear(self.clip.text_projection.out_features, proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(proj_dim, proj_dim)\n",
        "        )\n",
        "        if not train_image_encoder:\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pixel_values):\n",
        "        text_feats = self.clip.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_feats = self.txt_proj(text_feats)\n",
        "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        img_feats = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "        img_feats = self.img_proj(img_feats)\n",
        "        img_feats = img_feats / img_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return text_feats, img_feats\n",
        "\n",
        "def compute_contrastive_loss(text_feats, img_feats, K=3, temp=0.07):\n",
        "    B = text_feats.shape[0]\n",
        "    d = img_feats.shape[-1]\n",
        "    img_feats = img_feats.view(B, 1+K, d)\n",
        "    sims = torch.einsum('bd,bnd->bn', text_feats, img_feats)\n",
        "    logits = sims / temp\n",
        "    labels = torch.zeros(B, dtype=torch.long, device=logits.device)\n",
        "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "    preds = logits.argmax(dim=1)\n",
        "    acc = (preds == labels).float().mean().item()\n",
        "    return loss, acc\n",
        "\n",
        "def train_clip_reranker(train_items, val_items, epochs=10):\n",
        "    print('\\nðŸ”¥ Training CLIP Re-ranker...')\n",
        "    processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "    clip = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(DEVICE)\n",
        "\n",
        "    train_ds = CLIPReRankerDataset(train_items, processor, num_negatives=3)\n",
        "    val_ds = CLIPReRankerDataset(val_items, processor, num_negatives=3)\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "    reranker = ReRanker(clip, proj_dim=256, train_image_encoder=False).to(DEVICE)\n",
        "    params = [p for p in reranker.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(params, lr=5e-5, weight_decay=1e-4)\n",
        "    best_val_loss = float('inf')\n",
        "    K=3\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        reranker.train()\n",
        "        tloss, tacc, tsteps = 0, 0, 0\n",
        "        for batch in tqdm(train_loader, desc=f'CLIP Train {epoch+1}/{epochs}'):\n",
        "            if batch is None: continue\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            pv = batch['pixel_values'].to(DEVICE)\n",
        "            B, n_images, c, h, w = pv.shape\n",
        "            pv_flat = pv.view(B*n_images, c, h, w)\n",
        "            txt_feats, img_feats = reranker(input_ids, attention_mask, pv_flat)\n",
        "            loss, acc = compute_contrastive_loss(txt_feats, img_feats, K=K)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tloss += loss.item(); tacc += acc; tsteps += 1\n",
        "\n",
        "        reranker.eval()\n",
        "        vloss, vacc, vsteps = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f'CLIP Val {epoch+1}/{epochs}'):\n",
        "                if batch is None: continue\n",
        "                input_ids = batch['input_ids'].to(DEVICE)\n",
        "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "                pv = batch['pixel_values'].to(DEVICE)\n",
        "                B, n_images, c, h, w = pv.shape\n",
        "                pv_flat = pv.view(B*n_images, c, h, w)\n",
        "                txt_feats, img_feats = reranker(input_ids, attention_mask, pv_flat)\n",
        "                loss, acc = compute_contrastive_loss(txt_feats, img_feats, K=K)\n",
        "                vloss += loss.item(); vacc += acc; vsteps += 1\n",
        "\n",
        "        tloss /= max(1, tsteps); tacc /= max(1, tsteps)\n",
        "        vloss /= max(1, vsteps); vacc /= max(1, vsteps)\n",
        "        print(f'Epoch {epoch+1}: train_loss={tloss:.4f} train_acc={tacc:.3f} | val_loss={vloss:.4f} val_acc={vacc:.3f}')\n",
        "\n",
        "        if vloss < best_val_loss:\n",
        "            best_val_loss = vloss\n",
        "            torch.save(reranker.state_dict(), os.path.join(OUT_DIR, 'best_clip_reranker.pt'))\n",
        "            import pickle\n",
        "            with open(os.path.join(OUT_DIR, 'clip_processor.pkl'), 'wb') as f:\n",
        "                pickle.dump(processor, f)\n",
        "            print(f\"   -> New best model saved with validation loss: {vloss:.4f}\")\n",
        "\n",
        "    print('âœ… CLIP Re-ranker training completed!')\n",
        "    return reranker, processor\n",
        "\n",
        "def train_grounding_dino(train_items, val_items, epochs=5):\n",
        "    \"\"\"Fine-tune Grounding DINO model.\"\"\"\n",
        "    print('\\nðŸ”¥ Training Grounding DINO...')\n",
        "    model_id = \"IDEA-Research/grounding-dino-base\"\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(DEVICE)\n",
        "\n",
        "    train_ds = GroundingDINODataset(train_items)\n",
        "    val_ds = GroundingDINODataset(val_items)\n",
        "\n",
        "    collate_with_processor = partial(grounding_dino_collate_fn, processor=processor)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_with_processor)\n",
        "    val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=0, collate_fn=collate_with_processor)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad = 'bbox_embed' in name or 'class_embed' in name\n",
        "\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.AdamW(trainable_params, lr=2e-5, weight_decay=1e-4)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        tloss, tsteps = 0, 0\n",
        "\n",
        "        for i, batch in enumerate(tqdm(train_loader, desc=f'DINO Train {epoch+1}/{epochs}')):\n",
        "            if batch is None: continue\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            labels = []\n",
        "            for j in range(len(batch['pixel_values'])):\n",
        "                unique_phrases = batch['unique_phrases'][j]\n",
        "                phrase_map = {phrase: idx for idx, phrase in enumerate(unique_phrases)}\n",
        "                class_labels = [phrase_map[phrase] for phrase in batch['target_phrases'][j]]\n",
        "                labels.append({\n",
        "                    \"boxes\": batch['target_boxes'][j].to(DEVICE),\n",
        "                    \"class_labels\": torch.tensor(class_labels, device=DEVICE)\n",
        "                })\n",
        "\n",
        "            inputs = {\n",
        "                'pixel_values': batch['pixel_values'].to(DEVICE),\n",
        "                'input_ids': batch['input_ids'].to(DEVICE),\n",
        "                'attention_mask': batch['attention_mask'].to(DEVICE),\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                print(f\"\\n--- Debugging Batch {i} ---\")\n",
        "\n",
        "                for j in range(len(labels)):\n",
        "                    num_classes = len(batch['unique_phrases'][j])\n",
        "                    box_vals = labels[j]['boxes']\n",
        "                    label_vals = labels[j]['class_labels']\n",
        "\n",
        "                    print(f\"  Item {j}:\")\n",
        "                    print(f\"    - Unique Phrases: {batch['unique_phrases'][j]}\")\n",
        "                    print(f\"    - Num Classes: {num_classes}\")\n",
        "                    print(f\"    - Box tensor shape: {box_vals.shape}\")\n",
        "                    print(f\"    - Box values min/max: {box_vals.min():.4f} / {box_vals.max():.4f}\")\n",
        "                    print(f\"    - Label tensor shape: {label_vals.shape}\")\n",
        "                    print(f\"    - Label values min/max: {label_vals.min()} / {label_vals.max()}\")\n",
        "\n",
        "                    if label_vals.max() >= num_classes:\n",
        "                        print(\"    - ERROR: Max label index is out of bounds!\")\n",
        "                    if box_vals.min() < 0.0 or box_vals.max() > 1.0:\n",
        "                        print(\"    -  ERROR: Box coordinates are out of [0, 1] bounds!\")\n",
        "\n",
        "                print(\"--- Batch OK, sending to model... ---\")\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RUNTIME ERROR CAUGHT in Batch {i}. The printout above is for the failing batch.\")\n",
        "                raise e\n",
        "\n",
        "            loss = outputs.loss\n",
        "            if loss is not None:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss += loss.item(); tsteps += 1\n",
        "\n",
        "        print(f'Epoch {epoch+1}: train_loss={(tloss / max(1, tsteps)):.4f}')\n",
        "\n",
        "    print(' Grounding DINO training completed!')\n",
        "    return model, processor\n",
        "\n",
        "print('Training functions ready.')"
      ],
      "metadata": {
        "id": "2fdAo2JM9bNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 6: Data Preparation"
      ],
      "metadata": {
        "id": "XDNHYNMGALEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('\\n Starting Data Preparation...')\n",
        "\n",
        "\n",
        "print('\\n Parsing annotations...')\n",
        "all_items = parse_and_load_annotations()\n",
        "\n",
        "if len(all_items) == 0:\n",
        "    raise RuntimeError('No usable annotations found. Check your DRIVE_ROOT and folder structure!')\n",
        "\n",
        "print(f'Found {len(all_items)} annotated object instances.')\n",
        "\n",
        "\n",
        "with open(os.path.join(OUT_DIR, 'parsed_annotations.json'), 'w') as f:\n",
        "    json.dump(all_items, f, indent=2)\n",
        "\n",
        "image_to_items = {}\n",
        "for it in all_items:\n",
        "    image_to_items.setdefault(it['image_path'], []).append(it)\n",
        "\n",
        "image_paths = list(image_to_items.keys())\n",
        "train_imgs, val_imgs = train_test_split(image_paths, test_size=0.15, random_state=42)\n",
        "\n",
        "train_items = [it for im in train_imgs for it in image_to_items[im]]\n",
        "val_items = [it for im in val_imgs for it in image_to_items[im]]\n",
        "\n",
        "print(f'Dataset split: {len(train_items)} training items, {len(val_items)} validation items.')\n",
        "print('Data preparation complete.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FDAUWa039rK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 7: Train CLIP Re-ranker"
      ],
      "metadata": {
        "id": "5RRedvNXASxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_clip_reranker(train_items, val_items, epochs=8)"
      ],
      "metadata": {
        "id": "nXD3Cr4A9ucX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8: Train Grounding DINO"
      ],
      "metadata": {
        "id": "xlwbcXJ3AWD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_grounding_dino(train_items, val_items, epochs=5)"
      ],
      "metadata": {
        "id": "7nSUPUlH90J4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}